\documentclass[11pt,a4paper]{article}

\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage[parfill]{parskip}
\usepackage{float}
\usepackage{subcaption}

% Hyperlink configuration
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue,
    pdftitle={Star Classification and Clustering: A Comparative Analysis},
    pdfauthor={Marco Equisetto}
}

% Listings configuration
\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    breaklines=true,
    numbers=left,
    numberstyle=\tiny,
    frame=single,
    tabsize=4,
    captionpos=b,
    showstringspaces=false
}

\renewcommand{\topfraction}{0.9}      
\renewcommand{\bottomfraction}{0.8}   
\renewcommand{\textfraction}{0.07}    
\renewcommand{\floatpagefraction}{0.7} 
\setcounter{topnumber}{2}             
\setcounter{bottomnumber}{2}          
\setcounter{totalnumber}{4}           

\title{\textbf{Star Classification and Clustering: A Comparative Analysis} \\
       \large Machine Learning Project, Academic Year 2025/2026 \\
       University of Verona
       }
\author{Marco Equisetto\\VR535007}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
\noindent
This report presents an in-depth analysis of the Star Classification dataset, comparing the performance of multiple supervised and unsupervised models: it investigates how data preprocessing heavily impacts model outcomes and explores how different techniques can be combined to leverage their respective strengths.\\Three distinct supervised learning models are evaluated: \textbf{K-Nearest Neighbors} (\textbf{KNN}), \textbf{Random Forest Classifier} (\textbf{RFC}) and \textbf{Support Vector Machines} (\textbf{SVM}). Additionally, unsupervised learning techniques, specifically \textbf{K-Means} and \textbf{Gaussian Mixture Model} (\textbf{GMM}), are employed to explore the intrinsic topological nature of the astronomical data. Model performance is primarily evaluated using the F1-score, with the addition of accuracy, precision and recall for classification tasks, while Adjusted Rand Index (ARI), Normalized Mutual Info (NMI) and Silhouette Coefficient are utilized for clustering validation.
\end{abstract}

\tableofcontents
\newpage


\section{Introduction}
\label{sec:introduction}
The classification of celestial objects is a fundamental task in modern astrophysics. With the advent of large-scale sky surveys, and the constantly increasing amount of satellites and telescopes, the volume of spectral and photometric data has grown exponentially, creating the need to move from manual classification to \textit{machine learning} and \textit{deep learning} models \cite{sdss_paper}. Although machine learning models pale in comparison to deep learning ones in terms of ability to process high dimensionality and quantity data, they still offer a more than viable solution for rudimentary tasks such as the one tackled in this discussion.


\section{Motivation and Rationale}
\label{sec:motivation}
This project addresses the problem of classifying objects from the Sloan Digital Sky Survey (SDSS) \cite{sdss_paper} into three distinct classes: \textbf{Galaxies}, \textbf{Stars}, and \textbf{Quasars}. The rationale behind this work is to compare the efficacy of distance-based, ensemble, and linear models in handling astronomical data, which is often characterized by high dimensionality, considerable size and high levels of noise. Most of the currently employed models rely on classifying bodies by redshift and spectral indexes. This project turned such a task into one that relies \textbf{solely on color and emitted light}.


\section{State of the Art}
\label{sec:sota}
Astronomical classification has traditionally relied on the \textit{Morgan-Keenan} (MK) method, where each star has a spectral class and a luminosity class assigned to it. It has evolved from manual inspection to automatic pipelines capable of processing and handling data of orders of magnitude that are incomparable to those historically analyzed by hand. Current state-of-the-art methods generally fall into two categories depending on the input data format: ensemble methods for tabular photometric data and Deep Learning architectures for raw spectral or time-series data.\\For datasets consisting of extracted features (magnitudes, colors, redshift), much like the dataset used in this project, Gradient Boosted Decision Trees (GBDTs) are currently considered the accepted standard \cite{geron}, which is one of the reasons why Random Forest Classifier (RFC) was considered as a viable choice and why it was expected to performed quite well in this task. While tabular data with few selected features is efficient, the highest level of accuracy is obtained through the use of "raw" data, these being mathematical data like 1D-Spectra (a combination of intensity and wavelength) or light curves (brightness over time). Such data types are of complicated distributions and huge quantities, rendering them effectively almost impossible to handle by simple Machine Learning models, and therefore more suitable for Deep Learning models, such as Neural Networks.\\Despite all the technological advancements, the field of stellar classification still faces many hurdles, mainly:
\begin{itemize}
    \item \textbf{Imbalance}: The nature of the universe renders some objects much rarer than others (e.g. highly redshifted Quasars) and therefore data about said objects can be extremely hard to obtain, making it difficult to train models with equal representation of all classes.
    \item \textbf{Domain Shift}: Due to how much celestial objects can vary both inside and outside classes, training a model on one dataset with labeled data from one survey and then applying it to another dataset might result in poor performance due to the presence of different levels of noise profiles and instrument sensitivity, which is and always will be inevitable since data come from different measuring instruments.
\end{itemize}


\section{Objectives}
\label{sec:objectives}
The primary objective of this project is to develop a robust machine learning pipeline for celestial object classification and to find out which of the tested models best fits the task at hand. Specific objectives include:
\begin{enumerate}
    \item \textbf{Data Preprocessing:} To implement effective outlier removal, balancing and feature engineering (calculating color indices like $u-g$) to improve model separability and to remove problematic noisy features such as position based features (namely $alpha$ and $delta$).
    \item \textbf{Supervised Comparison:} To evaluate and tune hyperparameters for KNN, Random Forest, and SVM to find the best performing one for each of them.
    \item \textbf{Unsupervised Exploration:} To analyze the dataset using Clustering (K-Means, GMM) and assess the impact of Principal Component Analysis (PCA) on clustering performance.
\end{enumerate}


\section{Methodology}
\label{sec:methodology}
All experiments were conducted using Python, mainly utilizing the \texttt{scikit-learn} \cite{sklearn}, \texttt{pandas}, and \texttt{seaborn} \cite{vanderplas} libraries specifically, combined with other performance metric libraries. The single trainings of the various methods were developed separately and then brought together in one single train cycle, to standardize all of them and put all models in the same starting conditions, distribution and random seed generation.

\subsection{Dataset Description}
The dataset of choice is the \textit{Star Classification} dataset (sourced from Kaggle/SDSS) \cite{sdss}. It initially contains $100,000$ observations, with features like spectral columns ($u, g, r, i, z$), redshift, various IDs, spatial coordinates, namely $alpha$ and $delta$, and a target class, a categorical variable with three levels: \texttt{GALAXY}, \texttt{STAR}, \texttt{QSO}.

\subsection{Data Preprocessing and Feature Extraction}
To prepare the data for training, the following steps were taken:

\subsubsection{Cleaning}
The ID columns were dropped completely from the start since they do not provide real information and would very likely introduce biases or noise if kept.

\subsubsection{Missing Values Check}
Missing/zero values introduce sparsity and lower precision of models if many are present. In this specific dataset, the procedure did not detect any missing or 'zero' values, so no real removal action was needed and performed.

\subsubsection{Outlier Detection}
First, data coming from sensor malfunction was manually deleted (e.g., removing rows where $u = -9999$), then outliers were removed based on valid photometric ranges \cite{geron}. This detection was performed combining two different methods:
\begin{itemize}
    \item \textbf{Interquartile Range (IQR):} A rule that statistically defines a "reasonable" range in which data points could fall, anything out of which is considered an outlier. In this case, the line was traced based on a \textbf{1.5IQR rule} and only the data above (to the right of) such threshold was considered valid and kept.
    \item \textbf{Gaussian Mixture Model (GMM):} Applying a simple instance of GMM to initial data can show outliers as points which have a very low percentage probability to belong to any and all of the classes that are present, indicating that they are very likely outliers.
\end{itemize}
These techniques were combined and applied to produce the outlier detection result seen in Figure [\ref{fig:outliers_IQR_GMM}].

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{img/outliers.png}
    \caption{\textbf{Dataset Outliers}: On the left the distribution and where the threshold was placed by the 1.5IQR rule, effectively excluding all data to the left of it. On the right the GMM instance where outlier points were highlighted in red.}
    \label{fig:outliers_IQR_GMM}
\end{figure}

\subsubsection{Label Encoding}
Since the target label for classification was categorical (it being a string with the name of the object), a label encoder was used to convert it to a number. Keeping categorical, string-like features might have the model learn inexistent correlations based on the length of the word or specific letters. Encoding to a simple numeric ID removes the textual features entirely, leaving only the category identity.

\subsubsection{Balancing}
When considering whether the dataset needed balancing or not, the distribution of classes was analyzed: its result can be visualized in Figure [\ref{fig:piechart}].
These charts show that, even though there technically is an imbalance of representation, it is not strong enough to require direct action on the dataset itself. Utilizing the $class\_weight$ flag in models that contain it (such as RFC and SVM) will suffice to ensure each of them follows its own procedure to balance data.
As for the KNN model, since it can handle slight imbalance in a rather optimal way, no invasive balancing of the dataset felt strictly necessary.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{img/pie_chart.png}
    \caption{\textbf{Label Distribution}: How many entries are present inside each class represented as a percentage on the total amount of data.}
    \label{fig:piechart}
\end{figure}

\subsubsection{Feature Engineering}
The features of this dataset followed distributions that can be seen in Figure [\ref{fig:distribution}].
Based on correlation analysis, which was prompted by some of the implemented method's dire need of correlation-free environments to work at their best, said features produced the correlation matrix shown in Figure [\ref{fig:correlationMatrix1}], which implied some action needed to be taken to fix the high correlation between them.\\Instead of deleting all highly correlated features right away, a different approach was considered, which was to duplicate the dataset and create the two following versions:
\begin{itemize}
    \item \textbf{Tree-Based Set ($X_{tree}$):} Retains both raw magnitudes ($u, g, r, i, z$) and synthetic color indices (used for RFC)
    \item \textbf{Linear-Model Set ($X_{linear}$):} Drops the raw magnitudes and retains \textbf{only} the synthetic color indices to reduce multicollinearity (used for KNN, SVM and clustering).
\end{itemize}

This decision was taken because while models like KNN, SVM and clustering approaches see their performance hindered by correlated features, and in their case those need to be removed, RFC is effectively immune to such an even and therefore can benefit from both keeping the original features and introducing new information on top of it.
Correlation between the newly added features was evaluated and its result is seen in Figure [\ref{fig:correlationMatrix2}].

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{img/distributions.png}
    \caption{\textbf{Feature Distribution}: This plot visualizes how each of the post-preprocessing features is distributed inside each class. The most interesting feature is \textit{redshift}, as can be seen by its peculiar distribution with respect to the other ones.}
    \label{fig:distribution}
\end{figure}

\begin{figure}[htbp]
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/corr_matrix_original.png}
         \caption{\textbf{Initial Correlation}: Noticeable high correlation ($>0.9$) between several features.}
         \label{fig:correlationMatrix1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/corr_matrix_reduced.png}
         \caption{\textbf{Final Correlation}: Correlation between engineered features is much lower  (this is effectively the C.M. for the $X_{linear}$ dataset.)}
         \label{fig:correlationMatrix2}
     \end{subfigure}
     \caption{\textbf{Correlation Matrix Comparison}: The effect of feature engineering and deletion of highly correlated features}
     \label{fig:correlation_comparison}
\end{figure}


\subsubsection{Scaling}
A \texttt{StandardScaler} was applied to normalize features to zero mean and unit variance.

\subsubsection{Dimensionality Reduction}
For clustering analysis, \textbf{PCA} was applied to reduce the feature space to 2 principal components \cite{lectures}.

\section{Models Implemented}
The following supervised models were implemented using \textbf{5-Fold Cross-Validation} \cite{lectures}, with a \texttt{StandardScaler} applied within each pipeline to prevent data leakage.
\begin{itemize}
    \item \textbf{K-Nearest Neighbors (KNN):} Trained on the $X_{linear}$ dataset. Tuned for $k \in [1, 100]$.
    \item \textbf{Random Forest Classifier (RFC):} Trained on the $X_{tree}$ dataset. To optimize performance while handling class imbalance, the model utilized `class\_weight='balanced'` and a \textbf{GridSearchCV} was performed over the following parameter space:
    \begin{itemize}
        \item $n\_estimators \in [50, 150]$
        \item $max\_depth \in [None, 10, 20]$
        \item $min\_samples\_split \in [2, 5, 10]$
        \item $min\_samples\_leaf \in [1, 3, 5]$
    \end{itemize}
    \item \textbf{Support Vector Machine (SVM):} Trained on the $X_{linear}$ dataset with 
    $class\_weight='balanced'$. Due to the high computational cost of SVMs ($O(n^3)$), hyperparameter tuning was performed on a random \textbf{10\% subset} of the training data. The model was tuned for regularization $C \in [0.001, 0.01, 0.1, 1, 10, 100]$ and kernel type $kernel \in ['rbf', 'linear', 'sigmoid']$. The best configuration was then retrained on the full dataset.
\end{itemize}
For unsupervised learning, \textbf{K-Means} and \textbf{Gaussian Mixture Models (GMM)} were utilized with $k=3$ (or $n\_components=3$) to match the known number of classes. To test the hypothesis that dimensionality reduction aids cluster separation, both models were trained on two distinct versions of the data:
\begin{enumerate}
    \item \textbf{Raw Scaled Data:} The full $X_{linear}$ feature set.
    \item \textbf{PCA Reduced Data:} The $X_{linear}$ set projected onto 2 Principal Components ($n\_components=2$).
\end{enumerate}

\section{Experiments and Results}
\label{sec:results}
Each dataset was split into 80\% training and 20\% testing sets. The primary metric for model selection was the \textbf{Macro F1-Score} to account for potential class imbalances. The following are the most important ones to discuss.

\subsection{The Redshift Feature}
 An important observation needs to be made regarding the \textit{redshift} feature, that is the influence that it has on the performance of models. \textit{Redshift} is a phenomenon where the light (or other electromagnetic radiation) emitted by a celestial object is shifted toward the red end of the electromagnetic spectrum. This feature is heavily tied to the \textbf{Doppler Effect} and how light gets stretched by gravity, cosmological stretch and distance. By nature, celestial objects that emit light are highly characterized by this metric, and it is, in fact, a very telling feature: during development, it was discovered that even with very minimal corrective actions on the dataset, the performance of trained models that included these features were already on a degree of accuracy well above $0.97$, effectively meaning that the logical link between the class of the object and the redshift feature was so high that it was almost as if the data remained labeled even after removing the class feature during training.\\For the purpose of this project, and in order to create a bigger challenge, it was decided to also drop \textit{redshift} completely from the features included in the training and testing dataset. By doing so, classification became less trivial and was solely based on color, which was also a nice deviation from the SOTA.

\subsection{Synthetic Features}
 Removing the \textit{redshift} feature and $alpha$ and $delta$ features (position based features which have no meaningful information at all as to what object they refer to) meant that the model now only had very few color-based features to work with, namely:
 \begin{itemize}
    \item \textbf{u}: Ultraviolet filter in the photometric system
    \item \textbf{g}: Green filter in the photometric system
    \item \textbf{r}: Red filter in the photometric system
    \item \textbf{i}: Near Infrared filter in the photometric system
    \item \textbf{z}: Infrared filter in the photometric system
\end{itemize}

With some of these being highly correlated features (as shown in [\ref{fig:correlationMatrix1}]), the new challenge was to find a way to work with the few remaining and extrapolate meaningful data from them.\\This is when it was decided to create the following \textbf{synthetic features}:
\begin{itemize}
    \item \textbf{u\_g:} $u - g$ (\textbf{Ultraviolet-Green Index}: Sensitive to hot, young stars and Quasar UV-excess)
    \item \textbf{g\_r:} $g - r$ (\textbf{Green-Red Index}: A standard measure of surface temperature for main-sequence stars)
    \item \textbf{r\_i:} $r - i$ (\textbf{Red-Near Infrared Index}: Useful for detecting cooler, redder objects like M-dwarfs)
    \item \textbf{i\_z:} $i - z$ (\textbf{Near Infrared-Infrared Index}: Helps distinguish high-redshift galaxies)
 \end{itemize}
Introducing synthetic features means that models now have compound attributes with real-world meaning they can use during training and evaluation.
As mentioned previously, the engineered features were added to the already present ones for RFC and substituted for all the other implemented classification and clustering methods.

\subsection{Pipeline Training}
Each model was implemented inside its own pipeline, which handled training, fitting and evaluation. This solution made it possible to give each model the best chance to outperform the other ones, implementing further case-specific preprocessing and tuning on top of the commonly shared one.

\subsection{Supervised Learning Performance: Hyperparameter Tuning and Evaluation}
Cross-validation was used to find the optimal hyperparameters by training loops: each classification model was given incrementally higher hyperparameters and different combinations of them to work on the same dataset as all its predecessors. This method made it possible to investigate performance based solely on hyperparameter and not on change of data. Every model will be discussed separately.

\subsubsection{K-Nearest Neighbor Classifier}
The choice to explore the implementation of the KNN model came from how simple and straight forward it is and from how well it scales, training wise, with iteration on hyperparameter values: since it only requires one, iteration to decide its best value is efficient \cite{geron}. It also works well with data that is assumed to be distributed in a way that renders similar data points in close proximity to each other, which looked to be the case when performing the analysis of Figure [\ref{fig:outliers_IQR_GMM}]. Training loop result can be seen in Figure [\ref{fig:KNNtraining}].
Having obtained the best value for K, a new KNN model was trained specifically with this hyperparameter value to show evaluation metrics on the best version of KNN obtainable on this dataset. The obtained performance metrics were:
\begin{itemize}
    \item F1-Score: 0.8341
    \item Accuracy: 0.8736
    \item Precision: 0.8482
    \item Recall: 0.8312
\end{itemize}
The confusion matrix of KNN with $n\_neighbors = 37$ can be seen in Figure [\ref{fig:confusionMatrixKNN}].


\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/KNN_performance.png}
        \caption{\textbf{K in KNN}: F1-score grows to a highpoint at $BestK = 37$, where it starts decreasing. In this case, there is a clear winner as for best hyperparameter value.}
        \label{fig:KNNtraining}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/KNN_confm.png}
        \caption{\textbf{Confusion Matrix for KNN}}
        \label{fig:confusionMatrixKNN}
    \end{subfigure}
    \caption{\textbf{KNN Performance:} A look into how well KNN performed at its best.}
    \label{fig:KNN_metrics}
\end{figure}


\subsubsection{Random Forest Classifier}
RFC models avoid overfitting data extremely well by taking the majority vote from multiple individual trees. It also is extremely robus when presented with correlated features, hence the decision of feeding it the $X_{tree}$ dataset. To optimize the model, a \textbf{GridSearchCV} was performed to find the best combination of tree depth and split criteria.
The grid search identified the optimal hyperparameters, favoring a balance between complexity and generalization. This approach ensured that the trees were not only sufficient in number but also individually optimized to handle the raw and synthetic feature mix.
The graph at Figure [\ref{fig:RFC_feature_importance}] shows the feature importance, that is how much information gain was provided by each feature: the higher the bar is, the more that feature helped RFC separate data (these are the features that are higher up in the trees, closer to the root). The features on the lower end are those who were used primarily for tie-braking deep down the trees, mostly helping to refine special edge cases.
An in-depth visualization of this concept was provided in Figure [\ref{fig:optimizedTree}].

With the best hyperparameter configuration found by the grid search being: $max\_depth = 20$,  $min\_samples\_leaf = 3$, $min\_samples\_split = 2$, $n\_estimators = 150$. 
the specific optimally configurated model, when trained in isolation, produced the following evaluation metrics:
\begin{itemize}
    \item F1-Score: 0.8583
    \item Accuracy: 0.8887
    \item Precision: 0.8594
    \item Recall: 0.8565
\end{itemize}
The confusion matrix of RFC with $n\_estimators = 150$ can be seen in Figure [\ref{fig:confusionMatrixRFC}].

\begin{figure}[htbp]
    \begin{subfigure}[b]{0.48\textwidth}
         \centering
            \includegraphics[width=\textwidth]{img/feature_importance.png}
            \caption{\textbf{Feature Importance}: How much the features given to RFC helped classify.}
            \label{fig:RFC_feature_importance}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/RFC_confm.png}
        \caption{\textbf{Confusion Matrix for RFC}}
        \label{fig:confusionMatrixRFC}
    \end{subfigure}
    \caption{\textbf{RFC Performance:} A look into how well RFC performed at its best.}
    \label{fig:RFC_metrics}
\end{figure}

\begin{figure}[htbp] 
    \centering
    \includegraphics[width=1\textwidth]{img/optimized_tree.png}
    \caption{\textbf{Optimized Tree}: An in-depth view of the feature importance. The information is the same as the histogram, but here it is more apparent and easier to see how tree-like the organization of most important features really is: the closer to the root, the more discriminative a feature is.}
    \label{fig:optimizedTree}
\end{figure}


\subsubsection{Support Vector Machines}
SVM models have a double degree of freedom: different kernel modes and hyperparameter value. Performance and how well it can find an optimal hyperplane to separate classes in an n-dimensional space hugely vary based on them. Due to the high computational cost of training SVMs on large datasets, hyperparameter tuning was performed on a random 10\% subset of the training data. The best performing configuration found on the subset was then used to retrain the model on the full training dataset. The train loop for this model is displayed in Figure [\ref{fig:SVMtraining}].
It is apparent the best performing mode was $'rbf'$, which is the expected result: this indirectly confirms that the data is not linearly separable and therefore a linear kernel mode could have never performed well enough to surpass its competitors. RBF has the highest performance because it can sort of bend the separation line around data, better fitting the separation and achieving a higher precision and accuracy.\\The statistics that follow are the ones obtained by training a SVM with kernel mode 'rbf' and with $C = 10$:
\begin{itemize}
    \item F1-Score: 0.8092
    \item Accuracy: 0.8501
    \item Precision: 0.8087
    \item Recall: 0.8345
\end{itemize}
The confusion matrix of SVM with $kernel=rbf, C=10$ can be seen in Figure [\ref{fig:confusionMatrixSVM}].


\begin{figure}[htbp]
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/SVM_kernels.png}
        \caption{\textbf{Performance of SVM}: Comparison between kernel modes and how well they performed cycling through the $C$ values.}
        \label{fig:SVMtraining}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/SVM_confm.png}
        \caption{\textbf{Confusion Matrix for SVM}}
        \label{fig:confusionMatrixSVM}
    \end{subfigure}
    \caption{\textbf{SVM Performance:} A look into how well SVM performed at its best.}
    \label{fig:SVM_metrics}
\end{figure}



\subsubsection{Final Metrics}
From the analysis that was conducted, out of all the models that were trained and evaluated, the winner, that is the model that achieved the best results in terms of performance metrics, is the \textbf{Random Forest Classifier} model. A small table summarizing the results of all models was provided in Table [\ref{tab:results}].

\begin{table}[htbp]
    \centering
    \caption{Model Performance Comparison (Best Hyperparameter Versions)}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & \textbf{Accuracy} & \textbf{Precision (Macro)} & \textbf{Recall (Macro)} & \textbf{F1 (Macro)} \\
        \midrule
        KNN & 0.8736 & 0.8482 & 0.8312 & 0.8341 \\
        \textbf{Random Forest} & \textbf{0.8887} & \textbf{0.8594} & \textbf{0.8565} & \textbf{0.8583} \\
        SVM & 0.8501 & 0.8087 & 0.8345 & 0.8092 \\
        \bottomrule
    \end{tabular}
    \label{tab:results}
\end{table}


\subsection{Clustering Analysis}
Clustering performance was evaluated by comparing models trained on high-dimensional scaled data against those trained on a two-dimensional PCA projection. This comparison highlights a significant trade-off: while dimensionality reduction simplifies the feature space and does, in some way, help models produce a higher quality result, it does not inherently make the data more clusterable.\\A final table was compiled, listing all the obtained evaluation metric results, which is available at Table [\ref{tab:resultscomparison}].

\begin{figure}[htbp] 
    \centering \includegraphics[width=0.8\textwidth]{img/GMM_KMEANS_cluster.png} 
    \caption{\textbf{Clustering Visualization}: Comparison between K-Means and GMM trained on raw scaled features (left) versus PCA-reduced features (right). The PCA projection visually collapses the variance into distinct regions, though significant overlap remains.} 
    \label{fig:clusteringPerformance} 
\end{figure}

\begin{table}[htbp]
    \centering
    \caption{Raw vs PCA Clustering: comparison of K-Means and GMM}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & \textbf{ARI} & \textbf{NMI} & \textbf{Silhouette} \\
        \midrule
        K-Means (raw) & 0.1068 & 0.1904 & 0.3551 \\
        K-Means (PCA) & 0.1129 & 0.1919 & 0.4338 \\
        GMM (raw) & 0.0990 & 0.1466 & 0.1517 \\
        GMM (PCA) & 0.0652 & 0.1302 & 0.3894 \\
        \bottomrule
    \end{tabular}
    \label{tab:resultscomparison}
\end{table}

As illustrated in Figure [\ref{fig:clusteringPerformance}], PCA significantly improved the Silhouette Score for both models clustering models. This indicates that PCA successfully filtered out noise, creating more defined and separable spatial groupings. However, quantitative metrics reveal that this spatial clarity did not translate to meaningful classification: 
\begin{itemize} 
    \item \textbf{Weak Ground-Truth Alignment}: The Adjusted Rand Index (ARI) remained extremely low across all configurations, peaking at only $0.1129$ for K-Means on PCA data. This suggests that the natural clusters formed by the algorithms have very little overlap with the actual \textit{GALAXY, STAR,} and \textit{QSO} labels.
    \item \textbf{Information Loss in GMM}: Interestingly, while GMM's Silhouette score improved with PCA, its ARI actually dropped from $0.0990$ to $0.0652$. This indicates that the Gaussian components in the reduced 2D space lost useful density information present in the original feature set.
    \item \textbf{Conclusion on Viability}: With Normalized Mutual Info (NMI) values hovering below $0.20$, these features, even when optimized via PCA, are insufficient for unsupervised discovery of star classes. The data appears to not exist in discrete, well-separated density clusters.
\end{itemize}


\section{Conclusion}
\label{sec:conclusions}
This project set out to evaluate the performance of supervised and unsupervised machine learning models on the task of celestial object classification. A critical component of this study was the deliberate exclusion of the \textit{redshift} feature, forcing the models to rely solely on photometric color indices and reference magnitudes.

\subsection{Supervised Learning Synthesis}
The results demonstrate that while \textit{redshift} is the strongest predictor for cosmological classification, it is not strictly necessary for achieving reasonable accuracy in a controlled, machine learning context.
\begin{itemize}
    \item \textbf{Non-Linearity:} The fact that \textbf{K-Nearest Neighbors (KNN)} classifier and \textbf{Random Forest} outperformed the linear configurations of the SVM model confirms that the decision boundaries between Stars, Galaxies, and Quasars in color space ($u-g$, $g-r$, etc.) are highly non-linear and complex.
    \item \textbf{Robustness:} Although RFC does require more training time and computational expenses, it is by far the most reliable, robust and rewarding model out of the ones that were considered in this project: it is able to efficiently reach satisfactory solutions in terms of accuracy and does not overfit data.
\end{itemize}

\subsection{Unsupervised Learning Considerations}
The clustering analysis provided a crucial negative result. Despite the application of PCA to reduce noise, neither \textbf{K-Means} nor \textbf{GMM} performed well. This suggests that in the photometric feature space, Stars, Galaxies, and Quasars do not form distinct, separated blobs. Instead, they likely exist on a space where classes overlap significantly. This finding highlights the necessity of labeled data (Supervised Learning) for this specific astronomical task.

\subsection{Possible Future Enhancements}
While the removal of redshift provided a challenging testbed for these algorithms, modern astronomical surveys often require high precision. Future iterations of this work could explore the use of \textbf{Hierarchical Classification}, which separate Stars from Extragalactic objects first, then classifying Galaxies and Quasars, or \textbf{Deep Learning on Raw Spectra}, which utilizes Neural Networks on the raw spectral data rather than extracted tabular features to capture feature subtleties invisible to standard photometry. In conclusion, while it is possible to apply simple classification, the distribution of the data makes it impossible to employ rudimentary clustering methods expecting a very precise outcome.

\begin{thebibliography}{9}
\label{sec:refs}

\bibitem{sdss}
Sloan Digital Sky Survey (SDSS), \textit{Star Classification Dataset}, Kaggle Repository.

\bibitem{sklearn}
Pedregosa, F., et al., "Scikit-learn: Machine Learning in Python," \textit{Journal of Machine Learning Research}, 2011.

\bibitem{lectures}
Beyan, C., "Machine Learning Course Slides," University of Verona, A.Y. 2025/2026.

\bibitem{geron}
G{\'e}ron, A., "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow," \textit{O'Reilly Media}, 2019.

\bibitem{sdss_paper}
York, D. G., et al., "The Sloan Digital Sky Survey: Technical Summary," \textit{The Astronomical Journal}, 2000.

\bibitem{vanderplas}
VanderPlas, J., "Python Data Science Handbook: Essential Tools for Working with Data," \textit{O'Reilly Media}, 2016.

\end{thebibliography}

\end{document}