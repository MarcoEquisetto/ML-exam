Outliers are unreasonable values, with normal outlier detection the entire dataset is deleted, showing data has strange distribution but is valid and should not be deleted 

Was originally using accuracy as metric, changed to cross validation and F1 score for a more telling evaluation


decreased amount of K and estimators tested to 1 - 30 (included)

removed 'sigmoid' and 'precomputed' kernel mode from SVM due to poor performance


Tried to test LinearSVC specifically after seeing 'linearÃ¬ kernel was performing well (speed and accuracy wise)


Chose to test clustering algorithms due to the distribution of data. Will test K-means and GMM (Gaussian Mixture Model)

Check for overfitting or underfitting!

Am i working on test data and not on validation data? Specify it in the report if i split test into test and validation!


GMM is good to detect outliers, if a point has a very low percentage to belong to any cluster it means it is an outlier.


analisi con PCA per vedeere i componenti quanta variance aggiungono, poi valuto clustering su raw data e su PCA data.
Prendere feature poco leggibili e fare quello con pCA e poi con altri dati categorici


Drop redshift because its too good of a feature to classify and makes the models job too easy
due to this i have to do some feature engineering to go about correlated features but still keep crucial color information for my model's evaluation
-> Created synthetic featues to describe colors without the need to keep u g z 

Compared training with
    1. redshift + no synthetic features
    2. no redshift + synthetic features
and performance is better in case 2

Clustering is not influenced by the difference in case 1 or 2 since PCA selects the same features as best in either case


Added pipeline in training loops to fix mistake of data leakage (scaled the entire training set (X_train) before the cross-validation loop)
- Why it's bad: When cross_val_score splits X_train into internal folds, the validation fold has already influenced the scaler (mean/variance) 
used on the training fold. This "peeking" can lead to overly optimistic scores.

Clustering seems to perform poorly after evaluating metrics from both PCA-affected and raw data.
It is to note that, even if of poor performance, PCA did impact precision meaningfully, almost doubling silhouette score and performance in general.
Evidently, this dataset is not suitable for a clustering attemp.

KNN and RFC seem to perform well, achieving around 90% accuracy given a Hyperparameter that performs well for each.

Exchanged LinearSVC for SVC with rbf kernel to test if non linearly separable data is better analyzed with such a kernel as one last test for SVM

'alpha' and 'delta' features dropped by default because they are position features, which do not help classification at all and might hinder performance introducing noise

Added testing on raw features to show performance on them and motivate the choice to implement feature engineering